# dbt + Airflow + BigQuery | Data Pipeline Orchestration

[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=apache-airflow&logoColor=white)](https://airflow.apache.org/)
[![dbt](https://img.shields.io/badge/dbt-FF694B?style=for-the-badge&logo=dbt&logoColor=white)](https://www.getdbt.com/)
[![Google BigQuery](https://img.shields.io/badge/Google%20BigQuery-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)](https://cloud.google.com/bigquery)
[![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)

## ğŸ“‹ DescriÃ§Ã£o do Projeto

Pipeline de dados moderno que combina **Apache Airflow** para orquestraÃ§Ã£o de workflows com **dbt** (data build tool) para transformaÃ§Ãµes de dados no **Google BigQuery**. O projeto utiliza a biblioteca **Cosmos** para integraÃ§Ã£o nativa entre Airflow e dbt, proporcionando observabilidade completa e controle granular sobre as transformaÃ§Ãµes de dados.

**Caso de uso:** AutomaÃ§Ã£o de pipelines de dados em ambiente de produÃ§Ã£o com capacidade de processamento escalÃ¡vel, monitoramento de qualidade e versionamento de transformaÃ§Ãµes.

## ğŸ› ï¸ Tecnologias Utilizadas

| Tecnologia | VersÃ£o | FunÃ§Ã£o |
|------------|--------|---------|
| **Apache Airflow** | 12.7.1 | OrquestraÃ§Ã£o de workflows |
| **dbt Core** | 1.9.1 | TransformaÃ§Ãµes SQL e modelagem |
| **Astronomer Cosmos** | 1.10.2 | IntegraÃ§Ã£o Airflow + dbt |
| **Google BigQuery** | - | Data Warehouse |
| **Docker** | Latest | ContainerizaÃ§Ã£o |
| **Python** | 3.9+ | Linguagem principal |

## ğŸ“ Arquitetura do Projeto

```
dbt-airflow-02/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ simple-dag.py          # DAG principal com configuraÃ§Ã£o Cosmos
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ nome-do-seu-projeto/            # Projeto dbt com models e testes
â”œâ”€â”€ include/                   # Arquivos auxiliares
â”œâ”€â”€ plugins/                   # Plugins customizados Airflow
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dags/                  # Testes das DAGs
â”œâ”€â”€ airflow_settings.yaml      # ConfiguraÃ§Ãµes locais
â”œâ”€â”€ CLAUDE.md                  # DocumentaÃ§Ã£o tÃ©cnica
â””â”€â”€ requirements.txt           # DependÃªncias Python
```

## âš¡ Funcionalidades Principais

- **OrquestraÃ§Ã£o Automatizada**: DAGs Airflow com scheduling configurÃ¡vel
- **TransformaÃ§Ãµes dbt**: Models SQL organizados em camadas (staging, intermediate, marts)
- **IntegraÃ§Ã£o Cosmos**: ConversÃ£o automÃ¡tica de models dbt em tasks Airflow
- **Observabilidade**: Monitoramento completo via interface Airflow
- **Ambiente Isolado**: ContainerizaÃ§Ã£o Docker para consistÃªncia

## ğŸ”§ PrÃ©-requisitos

- **Docker** e **Docker Compose**
- **Conta Google Cloud** com BigQuery habilitado
- **Service Account** GCP com permissÃµes BigQuery

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. Clone o RepositÃ³rio
```bash
git clone https://github.com/seu-usuario/dbt-airflow.git
cd dbt-airflow
```

### 2. Configurar seu Projeto

#### ğŸ“‚ Estrutura de Projetos dbt
Os projetos dbt devem ser organizados em `dbt/nome-do-seu-projeto/`:
```
dbt/
â”œâ”€â”€ nome-do-seu-projeto/          # Seu projeto dbt principal
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ outro_projeto/       # Projetos adicionais (opcional)
â””â”€â”€ ...
```

#### âš™ï¸ ConfiguraÃ§Ãµes ObrigatÃ³rias no DAG
Edite `dags/simple-dag.py` com suas configuraÃ§Ãµes especÃ­ficas:

```python
profile_config_01 = ProfileConfig(
    profile_name="nome-do-seu-projeto",           # âœï¸ Nome do seu projeto dbt
    target_name="prod",                   # âœï¸ Ambiente (dev/prod)
    profile_mapping=GoogleCloudServiceAccountFileProfileMapping(
        conn_id="my_google_cloud_platform_connection",  # âœï¸ ID da conexÃ£o Airflow
        profile_args={
            "project": "seu-projeto-gcp-123",       # âœï¸ ID do projeto GCP
            "dataset": "prod",                      # âœï¸ Dataset BigQuery
            "keyfile": "/usr/local/airflow/dbt/nome-do-seu-projeto/service-account.json"  # âœï¸ Chave de serviÃ§o (preferencialmente na raiz do projeto dbt)
        }
    )
)

my_cosmos_dag = DbtDag(
    project_config=ProjectConfig(
        f"{airflow_home}/dbt/nome-do-seu-projeto",  # âœï¸ Caminho do projeto dbt
    ),
    dag_id="seu-dag-id",                    # âœï¸ ID Ãºnico da DAG
    schedule_interval="@daily",             # âœï¸ FrequÃªncia de execuÃ§Ã£o
    start_date=datetime(2023, 1, 1),        # âœï¸ Data de inÃ­cio
    tags=["seu_time", "etl"],              # âœï¸ Tags para organizaÃ§Ã£o
)
```

#### ğŸ”‘ Configurar Credenciais BigQuery
```bash
# Definir variÃ¡veis de ambiente para autenticaÃ§Ã£o
export DBT_ENV_SECRET_PROJECT_ID="seu-projeto-gcp"
export DBT_ENV_SECRET_CLIENT_EMAIL="sua-service-account@projeto.iam.gserviceaccount.com"
export DBT_ENV_SECRET_PRIVATE_KEY="sua-chave-privada"
# ... outras variÃ¡veis conforme CLAUDE.md
```

### 3. Iniciar Ambiente de Desenvolvimento
```bash
# Com Astronomer CLI (recomendado)
astro dev start
```

### 4. Acessar Interfaces
- **Airflow UI**: http://localhost:8080
  - UsuÃ¡rio: `admin` | Senha: `admin`
- **Postgres DB**: localhost:5432
  - Database: `postgres` | UsuÃ¡rio: `postgres` | Senha: `postgres`

## ğŸ’¡ Como Usar

### Executar Pipeline Completo
```bash
# Via Airflow UI
1. Acesse http://localhost:8080
2. Localize a DAG "simple-dag"
3. Ative o toggle da DAG
4. Clique em "Trigger DAG"
```

### Comandos dbt Locais
```bash
# Navegar para o seu projeto dbt
cd dbt/nome-do-seu-projeto

# Instalar dependÃªncias
dbt deps

# Executar todos os models
dbt run

# Executar testes
dbt test

# Gerar documentaÃ§Ã£o
dbt docs generate && dbt docs serve
```

### Monitoramento
```python
# ConfiguraÃ§Ã£o da DAG (dags/simple-dag.py)
schedule_interval="@daily"    # ExecuÃ§Ã£o diÃ¡ria
start_date=datetime(2023, 1, 1)
catchup=False                 # NÃ£o processar execuÃ§Ãµes antigas
default_args={"retries": 1}   # 1 tentativa em caso de falha
```

## ğŸ“Š Exemplo de ImplementaÃ§Ã£o

### Estrutura de um Model dbt
```sql
-- models/staging/stg_orders.sql
{{ config(materialized='table') }}

SELECT 
    order_id,
    customer_id,
    order_date,
    total_amount,
    {{ dbt_utils.generate_surrogate_key(['order_id', 'customer_id']) }} as order_key
FROM {{ ref('raw_orders') }}
WHERE order_date >= '2023-01-01'
```

### ConfiguraÃ§Ã£o Cosmos
```python
# IntegraÃ§Ã£o automÃ¡tica dbt â†’ Airflow
my_cosmos_dag = DbtDag(
    project_config=ProjectConfig(f"{airflow_home}/dbt/nome-do-seu-projeto"),
    profile_config=profile_config_01,
    execution_config=ExecutionConfig(
        dbt_executable_path=f"{airflow_home}/dbt_venv/bin/dbt"
    ),
    schedule_interval="@daily",
    dag_id="simple-dag",
    tags=["marketing", "etl"]
)
```

## ğŸ¯ Destaques TÃ©cnicos

- **Arquitetura de Dados Moderna**: ImplementaÃ§Ã£o ELT com dbt
- **DevOps para Dados**: Versionamento, testes e CI/CD
- **Escalabilidade**: Processamento distribuÃ­do BigQuery
- **Observabilidade**: Logging e monitoramento integrados
- **Qualidade de Dados**: Testes automatizados dbt
- **Versionamento Fixo**: Cosmos 1.10.2 e dbt-bigquery 1.9.1 para estabilidade

## ğŸ“ Checklist de ConfiguraÃ§Ã£o

Antes de executar o projeto, verifique:

- [ ] Projeto dbt criado em `dbt/nome-do-seu-projeto/`
- [ ] Arquivo `dags/simple-dag.py` configurado com seus dados
- [ ] Credenciais GCP definidas nas variÃ¡veis de ambiente
- [ ] Service Account com permissÃµes BigQuery
- [ ] ConexÃ£o Airflow configurada (`my_google_cloud_platform_connection`)

## ğŸ“ˆ PrÃ³ximos Passos

- [ ] Implementar testes de qualidade de dados
- [ ] Configurar alertas via Slack/Email
- [ ] Deploy em ambiente Cloud (Astronomer/Cloud Composer)
- [ ] Adicionar models de machine learning

## ğŸ“§ Contato

**Diego Rebalbi**
- ğŸ“§ Email: [seu-email@exemplo.com]
- ğŸ’¼ LinkedIn: [linkedin.com/in/seu-perfil]
- ğŸ™ GitHub: [github.com/seu-usuario]

---

â­ **Se este projeto foi Ãºtil, deixe uma estrela!**